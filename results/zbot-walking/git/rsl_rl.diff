--- git status ---
On branch main
Your branch is up to date with 'origin/main'.

Changes not staged for commit:
  (use "git add <file>..." to update what will be committed)
  (use "git restore <file>..." to discard changes in working directory)
	modified:   rsl_rl/runners/on_policy_runner.py

no changes added to commit (use "git add" and/or "git commit -a") 


--- git diff ---
diff --git a/rsl_rl/runners/on_policy_runner.py b/rsl_rl/runners/on_policy_runner.py
index 8904dac..f5d3e88 100644
--- a/rsl_rl/runners/on_policy_runner.py
+++ b/rsl_rl/runners/on_policy_runner.py
@@ -11,7 +11,7 @@ import time
 import torch
 from collections import deque
 
-import rsl_rl
+import rsl_rl as rsl_rl
 from rsl_rl.algorithms import PPO
 from rsl_rl.env import VecEnv
 from rsl_rl.modules import ActorCritic, ActorCriticRecurrent, EmpiricalNormalization
@@ -30,11 +30,15 @@ class OnPolicyRunner:
 
         # resolve dimensions of observations
         obs, extras = self.env.get_observations()
+        
         num_obs = obs.shape[1]
-        if "critic" in extras["observations"]:
+        if extras and "critic" in extras["observations"]:
             num_critic_obs = extras["observations"]["critic"].shape[1]
         else:
             num_critic_obs = num_obs
+    
+        print(self.alg_cfg)
+        print(type(self.alg_cfg))
         actor_critic_class = eval(self.policy_cfg.pop("class_name"))  # ActorCritic
         actor_critic: ActorCritic | ActorCriticRecurrent = actor_critic_class(
             num_obs, num_critic_obs, self.env.num_actions, **self.policy_cfg
@@ -43,9 +47,10 @@ class OnPolicyRunner:
         # resolve dimension of rnd gated state
         if "rnd_cfg" in self.alg_cfg:
             # check if rnd gated state is present
-            rnd_state = extras["observations"].get("rnd_state")
-            if rnd_state is None:
-                raise ValueError("Observations for they key 'rnd_state' not found in infos['observations'].")
+            if extras:
+                rnd_state = extras["observations"].get("rnd_state")
+                if rnd_state is None:
+                    raise ValueError("Observations for they key 'rnd_state' not found in infos['observations'].")
             # get dimension of rnd gated state
             num_rnd_state = rnd_state.shape[1]
             # add rnd gated state to config
@@ -57,8 +62,8 @@ class OnPolicyRunner:
         if "symmetry_cfg" in self.alg_cfg:
             # this is used by the symmetry function for handling different observation terms
             self.alg_cfg["symmetry_cfg"]["_env"] = env
-
         # init algorithm
+
         alg_class = eval(self.alg_cfg.pop("class_name"))  # PPO
         self.alg: PPO = alg_class(actor_critic, device=self.device, **self.alg_cfg)
 
@@ -83,6 +88,9 @@ class OnPolicyRunner:
 
         # Log
         self.log_dir = log_dir
+
+        print(f"Saving training results to {self.log_dir}")
+
         self.writer = None
         self.tot_timesteps = 0
         self.tot_time = 0
@@ -355,6 +363,9 @@ class OnPolicyRunner:
                                locs['start_iter'] + locs['num_learning_iterations'] - locs['it']):.1f}s\n"""
         )
         print(log_string)
+        if self.log_dir is not None:
+            with open(os.path.join(self.log_dir, "training.txt"), "a") as f:
+                f.write(log_string)
 
     def save(self, path: str, infos=None):
         # -- Save PPO model